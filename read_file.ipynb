{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "# try:\n",
    "#   import open3d as o3d\n",
    "# except:\n",
    "#   subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"open3d\"])\n",
    "#   import open3d as o3d\n",
    "import open3d as o3d\n",
    "from config import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read Points Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read ply file\n",
    "path = img_dir + \"/2level_2.ply\"\n",
    "pcd = o3d.io.read_point_cloud(path)\n",
    "# ply_file.paint_uniform_color([1, 0.706, 0])\n",
    "\n",
    "# visualize ply file\n",
    "# o3d.visualization.draw_geometries([pcd])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.28125   1.35645  -2.50977 ]\n",
      " [-2.27148   1.35742  -2.51172 ]\n",
      " [-2.26367   1.35938  -2.51563 ]\n",
      " ...\n",
      " [ 0.754395 -0.437988 -0.768555]\n",
      " [ 0.757813 -0.437988 -0.768555]\n",
      " [ 0.76123  -0.437988 -0.768555]]\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_xyz = np.asarray(pcd.points)\n",
    "print(n_xyz)\n",
    "print(\"----------------------------\")\n",
    "n_xyz[:, 0] > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False False False ...  True  True  True]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.10449 ,  0.943848, -1.74609 ],\n",
       "       [ 1.10938 ,  0.941406, -1.74121 ],\n",
       "       [ 1.11914 ,  0.942871, -1.74414 ],\n",
       "       ...,\n",
       "       [ 0.754395, -0.437988, -0.768555],\n",
       "       [ 0.757813, -0.437988, -0.768555],\n",
       "       [ 0.76123 , -0.437988, -0.768555]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def crop(pcd, max_x, max_y, max_z):\n",
    "  n_xyz = np.asarray(pcd.points)\n",
    "  mask_x = n_xyz[:, 0] > max_x\n",
    "  print(mask_x)\n",
    "  mask_y = n_xyz[:, 1] > max_y\n",
    "  mask_z = n_xyz[:, 2] > max_z\n",
    "  mask = mask_x & mask_y & mask_z\n",
    "  return n_xyz[mask]\n",
    "\n",
    "max_x = -2\n",
    "max_y = -1\n",
    "max_z = -2\n",
    "\n",
    "crop(pcd, max_x, max_y, max_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detect distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUBOID_EXTENT_METERS = 200\n",
    "\n",
    "METERS_BELOW_START = 5\n",
    "METERS_ABOVE_START = 30\n",
    "\n",
    "points = np.array([\n",
    "    ## These points lie inside the cuboid\n",
    "    [-2770.94365061042, 722.0595600050154, -20.004812609192445],\n",
    "    [-2755.94365061042, 710.0595600050154, -20.004812609192445],\n",
    "    [-2755.94365061042, 710.0595600050154, -15.004812609192445],\n",
    "\n",
    "    ## These points lie outside the cuboid\n",
    "    [-2755.94365061042 + CUBOID_EXTENT_METERS, 710.0595600050154, -15.004812609192445],\n",
    "    [-2755.94365061042, 710.0595600050154 + CUBOID_EXTENT_METERS, -15.004812609192445],\n",
    "  ]).reshape([-1, 3])\n",
    "\n",
    "point_cloud = o3d.geometry.PointCloud()\n",
    "point_cloud.points = o3d.utility.Vector3dVector(points)\n",
    "\n",
    "## Start point here corresponds to an ego vehicle position start in a point cloud\n",
    "start_position = {'x': -2755.94365061042, 'y': 722.0595600050154, 'z': -20.004812609192445}\n",
    "cuboid_points = getCuboidPoints(start_position)\n",
    "\n",
    "points = o3d.utility.Vector3dVector(cuboid_points)\n",
    "oriented_bounding_box = o3d.geometry.OrientedBoundingBox.create_from_points(points)\n",
    "point_cloud_crop = point_cloud.crop(oriented_bounding_box)\n",
    "\n",
    "# View original point cloud with the cuboid, all 5 points present\n",
    "o3d.visualization.draw_geometries([point_cloud, oriented_bounding_box])\n",
    "\n",
    "# View cropped point cloud with the cuboid, only 3 points present\n",
    "o3d.visualization.draw_geometries([point_cloud_crop, oriented_bounding_box])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71091805, 0.36624466, 0.5396815 ],\n",
       "       [0.79690357, 0.48723825, 0.92339057],\n",
       "       [0.41518979, 0.20390169, 0.41986006]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.random(size=(4, 3))\n",
    "mask = (a[:,2] > 0.2 ) & (a[:, 1]> 0.2)\n",
    "b = a[mask]\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "## License: Apache 2.0. See LICENSE file in root directory.\n",
    "## Copyright(c) 2015-2017 Intel Corporation. All Rights Reserved.\n",
    "\n",
    "###############################################\n",
    "##      Open CV and Numpy integration        ##\n",
    "###############################################\n",
    "\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from config import *\n",
    "\n",
    "# Create Trackbar for Threshold\n",
    "# cv2.namedWindow('Depth Image')\n",
    "cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "cv2.createTrackbar('FPS', 'RealSense', 0, 50, lambda x: x+1)\n",
    "\n",
    "# Configure depth and color streams\n",
    "pipeline = rs.pipeline()\n",
    "config = rs.config()\n",
    "\n",
    "# Get device product line for setting a supporting resolution\n",
    "pipeline_wrapper = rs.pipeline_wrapper(pipeline)\n",
    "pipeline_profile = config.resolve(pipeline_wrapper)\n",
    "device = pipeline_profile.get_device()\n",
    "device_product_line = str(device.get_info(rs.camera_info.product_line))\n",
    "\n",
    "found_rgb = False\n",
    "for s in device.sensors:\n",
    "    if s.get_info(rs.camera_info.name) == 'RGB Camera':\n",
    "        found_rgb = True\n",
    "        break\n",
    "if not found_rgb:\n",
    "    print(\"The demo requires Depth camera with Color sensor\")\n",
    "    exit(0)\n",
    "\n",
    "fps = cv2.getTrackbarPos('FPS', 'RealSense')\n",
    "print(fps)\n",
    "config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, fps)\n",
    "\n",
    "if device_product_line == 'L500':\n",
    "    config.enable_stream(rs.stream.color, 960, 540, rs.format.bgr8, fps)\n",
    "else:\n",
    "    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, fps)\n",
    "\n",
    "# Start streaming\n",
    "pipeline.start(config)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # print(pipeline)\n",
    "        # Wait for a coherent pair of frames: depth and color\n",
    "        frames = pipeline.wait_for_frames()\n",
    "        depth_frame = frames.get_depth_frame()\n",
    "        color_frame = frames.get_color_frame()\n",
    "        if not depth_frame or not color_frame:\n",
    "            continue\n",
    "        \n",
    "        # Convert images to numpy arrays\n",
    "        depth_image = np.asanyarray(depth_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Apply colormap on depth image (image must be converted to 8-bit per pixel first)\n",
    "        depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)\n",
    "\n",
    "        depth_colormap_dim = depth_colormap.shape\n",
    "        color_colormap_dim = color_image.shape\n",
    "\n",
    "        # If depth and color resolutions are different, resize color image to match depth image for display\n",
    "        if depth_colormap_dim != color_colormap_dim:\n",
    "            resized_color_image = cv2.resize(color_image, dsize=(depth_colormap_dim[1], depth_colormap_dim[0]), interpolation=cv2.INTER_AREA)\n",
    "            images = np.hstack((resized_color_image, depth_colormap))\n",
    "        else:\n",
    "            images = np.hstack((color_image, depth_colormap))\n",
    "        \n",
    "        # Flip camera image\n",
    "        images = cv2.flip(images, 1)\n",
    "\n",
    "        # Save images\n",
    "        import os\n",
    "        # Create directory if not exist\n",
    "        if not os.path.exists('images'):\n",
    "            os.mkdir('images')\n",
    "        \n",
    "        if count // 2 == 0:\n",
    "            if len(os.listdir(\"./images\"))!=10:\n",
    "                cv2.imwrite(\"./images/image_\" + str(count) + \".png\", images)    \n",
    "        count += 1\n",
    "\n",
    "        # Show images\n",
    "        cv2.namedWindow('RealSense', cv2.WINDOW_AUTOSIZE)\n",
    "        cv2.imshow('RealSense', images)\n",
    "        key = cv2.waitKey(1)\n",
    "        # Press esc or 'q' to close the image window\n",
    "        if key & 0xFF == ord('q') or key == 27:\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        \n",
    "finally:\n",
    "\n",
    "    # Stop streaming\n",
    "    pipeline.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
